"use strict";(self.webpackChunkais_project_github_io=self.webpackChunkais_project_github_io||[]).push([[545],{3905:(e,t,a)=>{a.d(t,{Zo:()=>c,kt:()=>h});var n=a(7294);function s(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){s(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,s=function(e,t){if(null==e)return{};var a,n,s={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(s[a]=e[a]);return s}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(s[a]=e[a])}return s}var l=n.createContext({}),p=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},c=function(e){var t=p(e.components);return n.createElement(l.Provider,{value:t},e.children)},d="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,s=e.mdxType,r=e.originalType,l=e.parentName,c=o(e,["components","mdxType","originalType","parentName"]),d=p(a),u=s,h=d["".concat(l,".").concat(u)]||d[u]||m[u]||r;return a?n.createElement(h,i(i({ref:t},c),{},{components:a})):n.createElement(h,i({ref:t},c))}));function h(e,t){var a=arguments,s=t&&t.mdxType;if("string"==typeof e||s){var r=a.length,i=new Array(r);i[0]=u;var o={};for(var l in t)hasOwnProperty.call(t,l)&&(o[l]=t[l]);o.originalType=e,o[d]="string"==typeof e?e:s,i[1]=o;for(var p=2;p<r;p++)i[p]=a[p];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},9419:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>m,frontMatter:()=>r,metadata:()=>o,toc:()=>p});var n=a(7462),s=(a(7294),a(3905));const r={sidebar_position:1,title:"Using the BNSA",sidebar_label:"Using the BNSA",lastUpdatedAt:"2023/06/01",author:"Jo\xe3o Paulo",showLastUpdateAuthor:!0,showLastUpdateTime:!0,last_update:{date:"2023/06/01",author:"Jo\xe3o Paulo"}},i="Using the BNSA",o={unversionedId:"Getting Started/basic use/BNSA",id:"Getting Started/basic use/BNSA",title:"Using the BNSA",description:"Access the Jupyter notebook with the code available here!",source:"@site/docs/Getting Started/basic use/BNSA.md",sourceDirName:"Getting Started/basic use",slug:"/Getting Started/basic use/BNSA",permalink:"/docs/Getting Started/basic use/BNSA",draft:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1,title:"Using the BNSA",sidebar_label:"Using the BNSA",lastUpdatedAt:"2023/06/01",author:"Jo\xe3o Paulo",showLastUpdateAuthor:!0,showLastUpdateTime:!0,last_update:{date:"2023/06/01",author:"Jo\xe3o Paulo"}},sidebar:"tutorialSidebar",previous:{title:"Instalation",permalink:"/docs/Getting Started/instalation"},next:{title:"Using the RNSA",permalink:"/docs/Getting Started/basic use/RNSA"}},l={},p=[{value:"Importing Binary Negative Selection Algorithm.",id:"importing-binary-negative-selection-algorithm",level:2},{value:"Randomly generating binary samples and splitting the data.",id:"randomly-generating-binary-samples-and-splitting-the-data",level:2},{value:"Function to generate binary samples",id:"function-to-generate-binary-samples",level:3},{value:"Data generation and separation",id:"data-generation-and-separation",level:3},{value:"Testing the model:",id:"testing-the-model",level:2},{value:"Confusion matrix",id:"confusion-matrix",level:2}],c={toc:p},d="wrapper";function m(e){let{components:t,...r}=e;return(0,s.kt)(d,(0,n.Z)({},c,r,{components:t,mdxType:"MDXLayout"}),(0,s.kt)("h1",{id:"using-the-bnsa"},"Using the BNSA"),(0,s.kt)("p",null,"Access the Jupyter notebook with the code available ",(0,s.kt)("a",{parentName:"p",href:"https://github.com/AIS-Package/aisp/blob/main/examples/BNSA/example_with_randomly_generated_dataset-en.ipynb"},"here"),"!"),(0,s.kt)("h2",{id:"importing-binary-negative-selection-algorithm"},"Importing Binary Negative Selection Algorithm."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"from aisp.NSA import BNSA\n")),(0,s.kt)("h2",{id:"randomly-generating-binary-samples-and-splitting-the-data"},"Randomly generating binary samples and splitting the data."),(0,s.kt)("h3",{id:"function-to-generate-binary-samples"},"Function to generate binary samples"),(0,s.kt)("p",null,"In this function, samples of binary data with a degree of similarity below a defined threshold s are generated. However, the first 10% of the data is generated randomly, without taking into account the value of s. Furthermore, when there are already samples, unique samples are generated for the new class, ensuring that the random samples generated are not duplicated in different classes."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"import numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef generate_samples(n_samples: int, n_features: int, s: float, x: None):\n   class_samples = []\n   while len(class_samples) < n_samples:\n     similarity = 0\n     sample_rand = np.random.randint(0, 2, size=(n_features))\n     if(len(class_samples) > max(int(n_samples * 0.1), 1)):\n       similarity = cdist(class_samples, np.expand_dims(sample_rand, axis=0), metric='hamming')[0, :]\n       if x is not None:\n         if similarity[0] <= s and not np.any(np.all(sample_rand == x, axis=1)):\n           class_samples.append(sample_rand)\n       elif similarity[0] <= s:\n         class_samples.append(sample_rand)\n     else:\n       class_samples.append(sample_rand)\n   return np.array(class_samples)\n")),(0,s.kt)("h3",{id:"data-generation-and-separation"},"Data generation and separation"),(0,s.kt)("p",null,"In this step, 1000 pieces of data are generated, 500 representing class 'x' and 500 representing class 'y'. Each die is made up of 20 dimensions. It is important to highlight that these data are created in such a way that they present a degree of similarity of 80%, that is, they share common characteristics. After generation, the data is separated into training and test sets."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'from sklearn.model_selection import train_test_split\n# Setting the seed to 121 to ensure the reproducibility of the generated data.\nnp.random.seed(121)\n# Generating samples for class "x".\nx = generate_samples(500, 20, 0.2, None)\n# Generating unique samples for class "y", different from samples present in class "x".\ny = generate_samples(500, 20, 0.2, x)\n# Adding columns containing the outputs (labels) of classes "x" and "y".\nx = np.hstack((x, np.full((x.shape[0], 1), \'x\')))\ny = np.hstack((y, np.full((y.shape[0], 1), \'y\')))\n# Merging the two vectors (classes "x" and "y") and randomizing the order of the samples.\nindex = np.random.permutation(x.shape[0]*2)\ndataset = np.vstack((x, y))[index]\n# Separating the characteristics (inputs) and the output classes (labels).\nsamples = dataset[:, :-1].astype(int)\noutput = dataset[:, -1]\n# Data separation for training and testing.\ntrain_x, test_x, train_y, test_y = train_test_split(samples, output, test_size=0.2, random_state=1234321)\n\n')),(0,s.kt)("h2",{id:"testing-the-model"},"Testing the model:"),(0,s.kt)("p",null,"Starting the model and applying it to randomly generated samples, the current configuration consists of 250 detectors with a differentiation rate of 30%."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n# Starting the model.\nnsa = BNSA(N=250, aff_thresh=0.34, seed=1234321, max_discards=10000)\n# Conducting the training:\nnsa.fit(X=train_x, y=train_y)\n# Visualization of classes with test samples.\nprev_y = nsa.predict(test_x)\n# Showing the accuracy of predictions for real data.\nprint(f"The accuracy is {accuracy_score(prev_y, test_y)}")\nprint(classification_report(test_y, prev_y))\n')),(0,s.kt)("p",null,"Output:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"\u2714 Non-self detectors for classes (x, y) successfully generated:  \u2507\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2507 500/500 detectors\nThe accuracy is 0.96\n              precision    recall  f1-score   support\n\n           x       0.95      0.97      0.96        90\n           y       0.97      0.95      0.96       110\n\n    accuracy                           0.96       200\n   macro avg       0.96      0.96      0.96       200\nweighted avg       0.96      0.96      0.96       200\n")),(0,s.kt)("h2",{id:"confusion-matrix"},"Confusion matrix"),(0,s.kt)("p",null,"Here is the confusion matrix, where the main diagonal represents correctly predicted samples and the secondary diagonal shows the false positives. Out of 200 test data points, there were 5 false positives for class x and 3 false positives for class y."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"# Generating the confusion matrix and plotting it graphically.\nmat = confusion_matrix(y_true=test_y, y_pred=prev_y)\nsns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, xticklabels=nsa.classes, yticklabels=nsa.classes)\nplt.xlabel('Real')\nplt.ylabel('Estimated')\nplt.show()\n")),(0,s.kt)("p",null,(0,s.kt)("img",{src:a(6505).Z,width:"447",height:"447"})))}m.isMDXComponent=!0},6505:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/matrixBNSA-23fbdccafcb637808e8ed6071fd47c8a.png"}}]);