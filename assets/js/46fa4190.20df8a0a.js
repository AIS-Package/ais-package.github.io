"use strict";(self.webpackChunkais_project_github_io=self.webpackChunkais_project_github_io||[]).push([[545],{3905:(e,t,a)=>{a.d(t,{Zo:()=>c,kt:()=>h});var n=a(7294);function s(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){s(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,s=function(e,t){if(null==e)return{};var a,n,s={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(s[a]=e[a]);return s}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(s[a]=e[a])}return s}var o=n.createContext({}),p=function(e){var t=n.useContext(o),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},c=function(e){var t=p(e.components);return n.createElement(o.Provider,{value:t},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var a=e.components,s=e.mdxType,r=e.originalType,o=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),m=p(a),d=s,h=m["".concat(o,".").concat(d)]||m[d]||u[d]||r;return a?n.createElement(h,i(i({ref:t},c),{},{components:a})):n.createElement(h,i({ref:t},c))}));function h(e,t){var a=arguments,s=t&&t.mdxType;if("string"==typeof e||s){var r=a.length,i=new Array(r);i[0]=d;var l={};for(var o in t)hasOwnProperty.call(t,o)&&(l[o]=t[o]);l.originalType=e,l[m]="string"==typeof e?e:s,i[1]=l;for(var p=2;p<r;p++)i[p]=a[p];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}d.displayName="MDXCreateElement"},9419:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>o,contentTitle:()=>i,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>p});var n=a(7462),s=(a(7294),a(3905));const r={sidebar_position:1,title:"Using the BNSA",sidebar_label:"Using the BNSA",lastUpdatedAt:"2023/06/01",author:"Jo\xe3o Paulo",showLastUpdateAuthor:!0,showLastUpdateTime:!0,last_update:{date:"2024/05/13",author:"Jo\xe3o Paulo"}},i="Applying the BNSA",l={unversionedId:"Getting Started/basic use/BNSA",id:"Getting Started/basic use/BNSA",title:"Using the BNSA",description:"The present example, available here, aims to demonstrate the application of the binary negative selection algorithm. This algorithm is used to classify samples with discrete characteristics.",source:"@site/docs/Getting Started/basic use/BNSA.md",sourceDirName:"Getting Started/basic use",slug:"/Getting Started/basic use/BNSA",permalink:"/docs/Getting Started/basic use/BNSA",draft:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1,title:"Using the BNSA",sidebar_label:"Using the BNSA",lastUpdatedAt:"2023/06/01",author:"Jo\xe3o Paulo",showLastUpdateAuthor:!0,showLastUpdateTime:!0,last_update:{date:"2024/05/13",author:"Jo\xe3o Paulo"}},sidebar:"tutorialSidebar",previous:{title:"Instalation",permalink:"/docs/Getting Started/instalation"},next:{title:"Using the RNSA",permalink:"/docs/Getting Started/basic use/RNSA"}},o={},p=[{value:"Importing the BNSA algorithm",id:"importing-the-bnsa-algorithm",level:2},{value:"Generating samples",id:"generating-samples",level:2},{value:"Training",id:"training",level:2},{value:"Evaluation",id:"evaluation",level:2}],c={toc:p},m="wrapper";function u(e){let{components:t,...r}=e;return(0,s.kt)(m,(0,n.Z)({},c,r,{components:t,mdxType:"MDXLayout"}),(0,s.kt)("h1",{id:"applying-the-bnsa"},"Applying the BNSA"),(0,s.kt)("p",null,"The present example, available here, aims to demonstrate the application of the binary negative selection algorithm. This algorithm is used to classify samples with discrete characteristics."),(0,s.kt)("p",null,"Access the Jupyter notebook with the code available ",(0,s.kt)("a",{parentName:"p",href:"https://github.com/AIS-Package/aisp/blob/main/examples/BNSA/example_with_randomly_generated_dataset-en.ipynb"},"here"),"!"),(0,s.kt)("h2",{id:"importing-the-bnsa-algorithm"},"Importing the BNSA algorithm"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"from aisp.NSA import BNSA\n")),(0,s.kt)("h2",{id:"generating-samples"},"Generating samples"),(0,s.kt)("p",null,"Algorithm training and testing needs data samples. Thus, for the demonstration, two random classes were generated, using the following function:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"import numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef generate_samples(n_samples: int, n_features: int, s: float, x: None):\n   class_samples = []\n   while len(class_samples) < n_samples:\n     similarity = 0\n     sample_rand = np.random.randint(0, 2, size=(n_features))\n     if(len(class_samples) > max(int(n_samples * 0.1), 1)):\n       similarity = cdist(class_samples, np.expand_dims(sample_rand, axis=0), metric='hamming')[0, :]\n       if x is not None:\n         if similarity[0] <= s and not np.any(np.all(sample_rand == x, axis=1)):\n           class_samples.append(sample_rand)\n       elif similarity[0] <= s:\n         class_samples.append(sample_rand)\n     else:\n       class_samples.append(sample_rand)\n   return np.array(class_samples)\n")),(0,s.kt)("hr",null),(0,s.kt)("p",null,"Each class will have 500 samples, with the minimum similarity between samples being 80% (s = 0.2). These classes will be separated into training (800 samples) and testing (200 samples) sets."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'from sklearn.model_selection import train_test_split\n# Setting the seed to 121 to ensure the reproducibility of the generated data.\nnp.random.seed(121)\n# Generating samples for class "x".\nx = generate_samples(500, 20, 0.2, None)\n# Generating unique samples for class "y", different from samples present in class "x".\ny = generate_samples(500, 20, 0.2, x)\n# Adding columns containing the outputs (labels) of classes "x" and "y".\nx = np.hstack((x, np.full((x.shape[0], 1), \'x\')))\ny = np.hstack((y, np.full((y.shape[0], 1), \'y\')))\n# Merging the two vectors (classes "x" and "y") and randomizing the order of the samples.\nindex = np.random.permutation(x.shape[0]*2)\ndataset = np.vstack((x, y))[index]\n# Separating the characteristics (inputs) and the output classes (labels).\nsamples = dataset[:, :-1].astype(int)\noutput = dataset[:, -1]\n# Data separation for training and testing.\ntrain_x, test_x, train_y, test_y = train_test_split(samples, output, test_size=0.2, random_state=1234321)\n\n')),(0,s.kt)("hr",null),(0,s.kt)("h2",{id:"training"},"Training"),(0,s.kt)("p",null,"The model is tuned through training patterns. In this application, negative selection will distribute, with a differentiation rate of 30%, 250 detectors across the input space."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},'from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n# Starting the model.\nnsa = BNSA(N=250, aff_thresh=0.30, seed=1234321, max_discards=10000)\n# Conducting the training:\nnsa.fit(X=train_x, y=train_y)\n# Visualization of classes with test samples.\nprev_y = nsa.predict(test_x)\n# Showing the accuracy of predictions for real data.\nprint(f"The accuracy is {accuracy_score(prev_y, test_y)}")\nprint(classification_report(test_y, prev_y))\n')),(0,s.kt)("p",null,"Output:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"\u2714 Non-self detectors for classes (x, y) successfully generated:  \u2507\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2507 500/500 detectors\nThe accuracy is 0.93\n              precision    recall  f1-score   support\n\n           x       0.93      0.91      0.92        90\n           y       0.93      0.95      0.94       110\n\n    accuracy                           0.93       200\n   macro avg       0.93      0.93      0.93       200\nweighted avg       0.93      0.93      0.93       200\n")),(0,s.kt)("hr",null),(0,s.kt)("h2",{id:"evaluation"},"Evaluation"),(0,s.kt)("p",null,"The model obtained an accuracy of 0.93 for the test set. The precision in each class, for both x and y, was also 0.93. This can be seen in the confusion matrix in Figure 1."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"# Generating the confusion matrix and plotting it graphically.\nmat = confusion_matrix(y_true=test_y, y_pred=prev_y)\nsns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, xticklabels=nsa.classes, yticklabels=nsa.classes)\nplt.xlabel('Real')\nplt.ylabel('Estimated')\nplt.show()\n")),(0,s.kt)("p",null,(0,s.kt)("img",{src:a(6505).Z,width:"447",height:"447"})))}u.isMDXComponent=!0},6505:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/matrixBNSA-12668a9bab5e9afa317b1159d07bc14d.png"}}]);